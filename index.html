<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs</title>

    <!-- SEO Meta Description -->
    <meta name="description" content="Filtering pretraining can prevent unsafe knowledge, doesn’t sacrifice general performance, and results in models that are resistant to tampering.">
    <meta name="keywords" content="LLM safety, AI safety, machine learning, data filtering, adversarial fine-tuning, open-weight models, tamper-resistant safeguards, pretraining data, EleutherAI, UK AISI, University of Oxford, deep learning, model security, CBRN defense">
    <meta name="author" content="Kyle O'Brien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman">
    <link rel="canonical" href="https://deepignorance.ai">

    <!-- Open Graph Meta Tags -->
    <meta property="og:title" content="Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards">
    <meta property="og:description" content="Filtering pretraining data prevents dangerous capabilities, doesn’t sacrifice general performance, and results in models that are resistant to tampering.">
    <meta property="og:image" content="https://deepignorance.ai/images/results.png">
    <meta property="og:url" content="https://deepignorance.ai">
    <meta property="og:type" content="website">
    <meta property="og:site_name" content="Deep Ignorance">

    <!-- Twitter Card Meta Tags -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards">
    <meta name="twitter:description" content="Filtering pretraining data prevents dangerous capabilities, doesn’t sacrifice general performance, and results in models that are resistant to tampering.">
    <meta name="twitter:image" content="https://deepignorance.ai/images/results.png">

    <link rel="stylesheet" href="styles.css">
    <link rel="icon" type="image/png" href="images/favicon.png">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;500;600;700&display=swap" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <h1>Deep Ignorance</h1>
            <p class="subtitle">Filtering Pretraining Data Builds Tamper-Resistant<br>Safeguards into Open-Weight LLMs</p>

            <nav>
                <a href="https://arxiv.org/abs/2508.06601" class="nav-link" target="_blank">
                    <img src="images/paper-icon.png" alt="Deep Ignorance research paper icon" class="paper-icon">
                    <strong>Paper</strong>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://huggingface.co/collections/EleutherAI/deep-ignorance-685441040d024a0fee593d68" class="nav-link" target="_blank">
                    <img src="images/hf-logo.svg" alt="Hugging Face logo - Access Deep Ignorance models and datasets" class="hf-logo">
                    <strong>Hugging Face</strong>
                </a>
                <span class="nav-separator">|</span>
                <a href="https://github.com/EleutherAI/deep-ignorance" class="nav-link" target="_blank">
                    <svg viewBox="0 0 98 96" xmlns="http://www.w3.org/2000/svg" class="github-logo">
                        <path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z" fill="currentColor"/>
                    </svg>
                    <strong>GitHub</strong>
                </a>
            </nav>

            <div class="authors">
                <div class="author-row">
                    <span class="author">Kyle O'Brien<sup>1*</sup></span>
                    <span class="author">Stephen Casper<sup>2*</sup></span>
                </div>
                <div class="author-row">
                    <span class="author">Quentin Anthony<sup>1</sup></span>
                    <span class="author">Tomek Korbak<sup>2</sup></span>
                    <span class="author">Robert Kirk<sup>2</sup></span>
                    <span class="author">Xander Davies<sup>2,3</sup></span>
                    <span class="author">Ishan Mishra<sup>2</sup></span>
                </div>
                <div class="author-row">
                    <span class="author">Geoffrey Irving<sup>2</sup></span>
                    <span class="author">Yarin Gal<sup>2,3</sup></span>
                    <span class="author">Stella Biderman<sup>1</sup></span>
                </div>
            </div>
            <div class="affiliations">
                <div><sup>1</sup><strong>EleutherAI</strong> &nbsp;&nbsp; <sup>2</sup><strong>UK AISI</strong> &nbsp;&nbsp; <sup>3</sup><strong>University of Oxford</strong></div>
                <div class="equal-contrib"><sup>*</sup>Equal Contribution</div>
                <div class="emails">kyle@eleuther.ai &nbsp;&nbsp; scasper@mit.edu</div>
            </div>
        </header>

        <div class="figure">
            <img src="images/results.png" alt="Graph showing LLM performance: filtered models maintain general capabilities while resisting adversarial fine-tuning on biothreat knowledge" class="responsive-img">
            <p class="caption"><strong>Training data filtering makes LLMs resistant to adversarial fine-tuning without sacrificing general performance.</strong> Models whose training data has been filtered to remove text related to dual-use biology topics (left) have unaffected general capabilities and (right) have low biothreat proxy capabilities and resist up to 10,000 steps and 300M tokens of adversarial fine-tuning.</p>
        </div>

        <div class="quote-block">
            <blockquote>
                <p>"Open weights allow global research communities to both advance capabilities and address model flaws by providing them with direct access to a critical AI component that is prohibitively expensive for most actors to develop independently. However, the open release of model weights could also pose risks of facilitating malicious or misguided use or perpetuating model flaws and biases. Once model weights are available for public download, there is no way to implement a wholesale rollback of all existing copies of the model."</p>
                <cite>— <a href="https://assets.publishing.service.gov.uk/media/679a0c48a77d250007d313ee/International_AI_Safety_Report_2025_accessible_f.pdf" target="_blank">International AI Safety Report</a> (Bengio et al., 2025)</cite>
            </blockquote>
        </div>

        <main>
            <section id="introduction">
                <h2>Introduction</h2>

                <p>
                    A risk as LLMs grow more capable is that they learn unsafe knowledge during training. These dangerous capabilities could be exploited by bad actors. Today's safeguards focus on suppressing unsafe knowledge in post-training, often via refusal training. The unsafe knowledge remains in the model's weights.
                </p>
                <p>
                    Open-weight models, those that users can download and modify locally, offer unique benefits related to transparency, research, and the deconcentration of power. However, they are vulnerable to "tampering attacks" that can remove their safety training. For instance, it is straightforward to train open-weight models never to refuse unsafe requests. This is of increasing concern as open-weight models begin to rival the capabilities of the best closed-weight models.
                </p>
                <p>
                    We explore an intuitive yet understudied question: Can we prevent LLMs from learning unsafe technical capabilities (such as CBRN) by filtering out enough of the relevant pretraining data before we begin training a model? We train multiple 6.9B LLMs from scratch on an unfiltered dataset and on filtered versions where we filtered out biorisk knowledge. We observe three main results:
                </p>
                <ol>
                    <li><strong>Knowledge Prevention:</strong> The filtered models perform significantly worse on our biorisk knowledge evaluations, nearly at random chance. Crucially, filtering does not lead to notable regressions in general knowledge. These results suggest that data filtering may be a simple way to prevent models from learning dangerous capabilities without sacrificing utility.</li>
                    <li><strong>Tamper-Resistance:</strong> Open-weight models can be fine-tuned by downstream users on biorisk data. We study this attack by fine-tuning our models on 300M tokens of high-quality biorisk-related documents. We find that performance can improve, but that it is still well below the no-filtering baseline. Data filtering is significantly more tamper-resistant than current safeguards.</li>
                    <li><strong>Defense-in-Depth:</strong> We demonstrate that data filtering cannot prevent LLMs from leveraging harmful knowledge provided in-context, but that Circuit-Breaking-based techniques offer complementary defenses. However, we show that none of the defenses we test are resistant to staged attacks that combine fine-tuning and in-context retrieval.</li>
                </ol>
                <p>
                    <strong>Taken together, these results suggest that rigorous pretraining data filtering is a promising method for preventing acquisition of dangerous technical capabilities without obvious degradation in overall model utility.</strong> Our efficient data approach allowed us to perform filtering with less than a 1% increase in training compute (FLOPS). We release our models, optimizer states, and intermediate checkpoints to enable future research into domains including data-driven AI security, pretraining research, machine unlearning, and mechanistic interpretability. We are especially excited to see future work that addresses our limitations, such as studying the filtering of other types of knowledge and developing scaling trends. See our paper for more details, discussion, and sketches of future work.
                </p>

                <div class="figure">
                    <img src="images/filtering_method.png" alt="Diagram of multi-stage data filtering pipeline: blocklist filtering followed by classifier evaluation for biothreat content removal" class="responsive-img">
                    <p class="caption"><strong>Our multi-stage data filtering pipeline: Our goal is to filter out data related to unwanted topics.</strong> We study biothreat-proxy knowledge as a representative example. All documents undergo initial "blocklist" filtering, where those without prohibited terms are retained without further review. Documents containing blocked terms (e.g., "pathogen(s)") are escalated to a fine-tuned text classifier that evaluates semantic content. The classifier assigns probability scores for unsafe content: documents scoring below the predetermined threshold are retained, while those exceeding it are excluded from the training corpus. In practice, the vast majority of documents are approved by the blocklist and thus do not require review by the classifier stage.</p>
                </div>
            </section>

            <section id="articles & press">
                <h2>Articles & Press</h2>
                <div class="press-items">
                    <a href="https://www.washingtonpost.com/newsletter/politics/2025/08/12/ai-systems-ignorant-sensitive-data-can-be-safer-still-smart/" target="_blank" class="press-item">
                        <img src="images/wapo_logo.png" alt="The Washington Post logo" class="press-logo">
                        <div class="press-content">
                            <strong>The Washington Post</strong><br>
                            AI systems ignorant of sensitive data can be safer but still smart
                        </div>
                    </a>
                    <a href="https://www.ox.ac.uk/news/2025-08-12-study-finds-filtered-data-stops-openly-available-ai-models-performing-dangerous" target="_blank" class="press-item">
                        <img src="images/oxford_big_logo.png" alt="University of Oxford logo" class="press-logo">
                        <div class="press-content">
                            <strong>University of Oxford</strong><br>
                            Study finds filtered data stops openly available AI models from performing dangerous tasks
                        </div>
                    </a>
                    <a href="https://blog.eleuther.ai/deep-ignorance/" target="_blank" class="press-item">
                        <img src="images/EleutherAI_big_logo.png" alt="EleutherAI logo" class="press-logo">
                        <div class="press-content">
                            <strong>EleutherAI Blog</strong><br>
                            Pretraining Data Filtering for Open-Weight AI Safety
                        </div>
                    </a>
                    <a href="https://jack-clark.net/" target="_blank" class="press-item">
                        <img src="images/importai_logo.webp" alt="Import AI logo" class="press-logo">
                        <div class="press-content">
                            <strong>Jack Clark's Import AI</strong><br>
                            Want to make your open weight AI system safer? Remove the dangerous data from the pre-training mix
                        </div>
                    </a>
                </div>
            </section>

            <section id="artifacts">
                <h2>Released Artifacts</h2>
                <p>
                    All models and datasets are available on our <a href="https://huggingface.co/collections/EleutherAI/deep-ignorance-685441040d024a0fee593d68" target="_blank">HuggingFace collection</a>. These artifacts enable future research in data-driven AI security, pretraining methodologies, machine unlearning, and mechanistic interpretability. By releasing models with varying levels of filtering and post-training safeguards, we provide researchers with a comprehensive testbed for studying the causal impact of training data on model behavior and safety.
                </p>

                <div class="table-container">
                    <table class="artifacts-table">
                        <thead>
                            <tr>
                                <th>Model Name</th>
                                <th>Description</th>
                                <th>Filtering Strategy</th>
                                <th>Defense Type</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="section-header">
                                <td colspan="4"><strong>Baseline Models</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-unfiltered">deep-ignorance-unfiltered</a></td>
                                <td>Baseline model without filtering</td>
                                <td>None</td>
                                <td>None</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-pretraining-stage-unfiltered">deep-ignorance-pretraining-stage-unfiltered</a></td>
                                <td>Pretraining checkpoint (500B tokens)</td>
                                <td>None</td>
                                <td>None</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Core Filtered Models</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter">deep-ignorance-e2e-strong-filter</a></td>
                                <td>End-to-end strong filtering</td>
                                <td>Blocklist only (8.42% removed)</td>
                                <td>Data filtering</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-weak-filter">deep-ignorance-e2e-weak-filter</a></td>
                                <td>End-to-end weak filtering</td>
                                <td>Blocklist + ModernBERT</td>
                                <td>Data filtering</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-extra-weak-filter">deep-ignorance-e2e-extra-weak-filter</a></td>
                                <td>End-to-end extra weak filtering</td>
                                <td>Blocklist + ModernBERT (relaxed)</td>
                                <td>Data filtering</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Hybrid Filtering Models</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-strong-filter-pt-weak-filter-anneal">deep-ignorance-strong-filter-pt-weak-filter-anneal</a></td>
                                <td>Hybrid approach</td>
                                <td>Strong pretraining, weak annealing</td>
                                <td>Data filtering</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-weak-filter-pt-strong-filter-anneal">deep-ignorance-weak-filter-pt-strong-filter-anneal</a></td>
                                <td>Reverse hybrid approach</td>
                                <td>Weak pretraining, strong annealing</td>
                                <td>Data filtering</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Circuit Breaking (CB) Variants</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-unfiltered-cb">deep-ignorance-unfiltered-cb</a></td>
                                <td>Baseline + Circuit Breaking</td>
                                <td>None</td>
                                <td>CB post-training</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-cb">deep-ignorance-e2e-strong-filter-cb</a></td>
                                <td>Strong filter + Circuit Breaking</td>
                                <td>Blocklist only</td>
                                <td>Data filtering + CB</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-strong-filter-pt-weak-filter-anneal-cb">deep-ignorance-strong-filter-pt-weak-filter-anneal-cb</a></td>
                                <td>Hybrid + Circuit Breaking</td>
                                <td>Strong PT, weak anneal</td>
                                <td>Data filtering + CB</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>CB + Latent Adversarial Training (LAT) Variants</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-unfiltered-cb-lat">deep-ignorance-unfiltered-cb-lat</a></td>
                                <td>Baseline + CB + LAT</td>
                                <td>None</td>
                                <td>CB + LAT post-training</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-cb-lat">deep-ignorance-e2e-strong-filter-cb-lat</a></td>
                                <td>Strong filter + CB + LAT</td>
                                <td>Blocklist only</td>
                                <td>Maximum defense</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-strong-filter-pt-weak-filter-anneal-cb-lat">deep-ignorance-strong-filter-pt-weak-filter-anneal-cb-lat</a></td>
                                <td>Hybrid + CB + LAT</td>
                                <td>Strong PT, weak anneal</td>
                                <td>Maximum defense</td>
                            </tr>

                            <tr class="section-header">
                                <td colspan="4"><strong>Knowledge Corruption Variants</strong></td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-weak-knowledge-corrupted">deep-ignorance-e2e-strong-filter-weak-knowledge-corrupted</a></td>
                                <td>Strong filter + weak corruption</td>
                                <td>Blocklist + synthetic corruption</td>
                                <td>Data filtering + corruption</td>
                            </tr>
                            <tr>
                                <td><a href="https://huggingface.co/EleutherAI/deep-ignorance-e2e-strong-filter-strong-knowledge-corrupted">deep-ignorance-e2e-strong-filter-strong-knowledge-corrupted</a></td>
                                <td>Strong filter + strong corruption</td>
                                <td>Blocklist + radical corruption</td>
                                <td>Data filtering + corruption</td>
                            </tr>
                        </tbody>
                    </table>
                </div>

            </section>

            <section id="citation">
                <h2>Citation</h2>
                <p>If you use this work in your research, please cite:</p>

                <div class="citation-box">
                    <pre><code>@article{obrien2025deepignorance,
    title={Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs},
    author={O'Brien, Kyle and Casper, Stephen and Anthony, Quentin and Korbak, Tomek and Kirk, Robert and Davies, Xander and Mishra, Ishan and Irving, Geoffrey and Gal, Yarin and Biderman, Stella},
    journal={arXiv preprint arXiv:2508.06601},
    year={2025}
}</code></pre>
                </div>
            </section>
        </main>

        <footer>
            <p>Contact: <a href="https://kyleobrien.io/" target="_blank">Kyle O'Brien</a> | <a href="https://stephencasper.com/" target="_blank">Stephen Casper</a></p>
        </footer>
    </div>
</body>
</html>
